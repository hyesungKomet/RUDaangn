# streamlit_app.py
import os
import streamlit as st
import pandas as pd
import requests
from dateutil.relativedelta import relativedelta
import urllib3
from concurrent.futures import ThreadPoolExecutor, as_completed

# í˜ì´ì§€ ë ˆì´ì•„ì›ƒ
st.set_page_config(layout="wide")
st.title("ARE YOU DAANGN?")

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "is_searching" not in st.session_state:
    st.session_state.is_searching = False
if "stop_search" not in st.session_state:
    st.session_state.stop_search = False
if "results_df" not in st.session_state:
    st.session_state.results_df = pd.DataFrame()

# --- 1) ì§€ì—­ ì½”ë“œ CSV ë¡œë“œ ---
@st.cache_data
def load_regions():
    df = pd.read_csv("address_with_all_codes.csv", dtype=str)
    df = df.drop_duplicates(subset=["ê²€ìƒ‰ì–´", "region_name", "region_code"]).copy()
    # ê´‘ì—­ ì»¬ëŸ¼ ìƒì„± (ê²€ìƒ‰ì–´ì˜ ì²« ë²ˆì§¸ ë‹¨ì–´)
    df["ê´‘ì—­"] = df["ê²€ìƒ‰ì–´"].map(lambda x: x.split()[0])
    MAJORS = [
        "ì„œìš¸íŠ¹ë³„ì‹œ","ë¶€ì‚°ê´‘ì—­ì‹œ","ëŒ€êµ¬ê´‘ì—­ì‹œ","ì¸ì²œê´‘ì—­ì‹œ","ê´‘ì£¼ê´‘ì—­ì‹œ",
        "ëŒ€ì „ê´‘ì—­ì‹œ","ìš¸ì‚°ê´‘ì—­ì‹œ","ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ","ê²½ê¸°ë„","ê°•ì›íŠ¹ë³„ìì¹˜ë„",
        "ì¶©ì²­ë¶ë„","ì¶©ì²­ë‚¨ë„","ì „ë¼ë¶ë„","ì „ë¼ë‚¨ë„","ê²½ìƒë¶ë„","ê²½ìƒë‚¨ë„","ì œì£¼íŠ¹ë³„ìì¹˜ë„"
    ]
    return df, MAJORS

regions_df, MAJORS = load_regions()

# --- 2) ê²€ìƒ‰ íŒŒë¼ë¯¸í„° UI ---
row1_col1, row1_col2, row1_col3 = st.columns([1, 1, 4])
with row1_col1:
    mode = st.radio("ğŸ” ëª¨ë“œ", ["ì „êµ­ ê²€ìƒ‰", "ì§€ì—­ ê²€ìƒ‰"], index=1, horizontal=True)
with row1_col2:
    # --- UI: ë³‘ë ¬ ê²€ìƒ‰ ì˜µì…˜ ì¶”ê°€ ---
    parallel = st.checkbox(
        "ë¹ ë¥¸ ê²€ìƒ‰ ì‚¬ìš©",
        value=False,
        help="ì•„ì§ ì‹¤í—˜ì¤‘..."
    )
with row1_col3:
    item = st.text_input("ì°¾ì„ ë¬¼í’ˆ (ex: ë…¸íŠ¸ë¶)", "")

row2_cols = st.columns([3, 2, 1, 1, 1])
with row2_cols[0]:
    kws = st.text_input(
        "í•µì‹¬ í‚¤ì›Œë“œ - ì‰¼í‘œë¡œ êµ¬ë¶„ (ex: ê²Œì´ë°, 3060)",
        help="ì œëª©+ë‚´ìš©ì— ëª¨ë‘ í¬í•¨ëœ í•­ëª©ë§Œ í•„í„°ë§"
    )
    keywords = [w.strip().lower() for w in kws.split(",") if w.strip()]
with row2_cols[1]:
    period = st.selectbox(
        "ë“±ë¡ ê¸°ê°„",
        ["ì „ì²´","1ì¼","7ì¼","1ê°œì›”","3ê°œì›”","6ê°œì›”","1ë…„"],
        help="ê¸°ê°„ë³„ ìµœê·¼ ë“±ë¡ê¸€ë§Œ ê²€ìƒ‰"
    )
with row2_cols[2]:
    only_available = st.checkbox(
        "íŒë§¤ì¤‘ë§Œ ë³´ê¸°", value=False,
        help="íŒë§¤ì™„ë£Œëœ ê¸€ ì œì™¸"
    )
with row2_cols[3]:
    min_price = st.number_input(
        "ìµœì†Œ ê°€ê²©", value=0, step=1000,
        help="ì› ë‹¨ìœ„ë¡œ ìµœì†Œ ê°€ê²© ì„¤ì •"
    )
with row2_cols[4]:
    max_price = st.number_input(
        "ìµœëŒ€ ê°€ê²©", value=0, step=1000,
        help="0ì´ë©´ ì œí•œ ì—†ìŒ"
    )

# ì»·ì˜¤í”„ ê³„ì‚° í•¨ìˆ˜ (dateutil.relativedelta ì‚¬ìš©)
def compute_cutoff(period):
    now = pd.Timestamp.now(tz="Asia/Seoul")
    if period == "1ì¼": return now - relativedelta(days=1)
    if period == "7ì¼": return now - relativedelta(days=7)
    if period == "1ê°œì›”": return now - relativedelta(months=1)
    if period == "3ê°œì›”": return now - relativedelta(months=3)
    if period == "6ê°œì›”": return now - relativedelta(months=6)
    if period == "1ë…„": return now - relativedelta(years=1)
    return None

cutoff = compute_cutoff(period)

# --- 3) ì§€ì—­ ê²€ìƒ‰ í•„í„° ---
sub_sel = []
if mode == "ì§€ì—­ ê²€ìƒ‰":
    with st.expander("ì§€ì—­ í•„í„° ì„¤ì •", expanded=True):
        maj_sel = st.multiselect("ê´‘ì—­ ì„ íƒ", MAJORS, default=MAJORS[:3])
        sub_df = regions_df[regions_df["ê´‘ì—­"].isin(maj_sel)]
        sub_opts = sorted(sub_df["ê²€ìƒ‰ì–´"].unique())
        sub_sel = st.multiselect("ë™/ì/ë©´ ì„ íƒ (ë¹„ì›Œë‘ë©´ ëª¨ë‘)", sub_opts)
        if not sub_sel:
            sub_sel = sub_opts.copy()

# --- 4) í˜ì´ì§€ë„¤ì´ì…˜ ---
pag1, pag2 = st.columns(2)
with pag1:
    per_page = st.number_input(
        "í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜",
        1, 100, 10, 1,
        help="limit: í•œ í˜ì´ì§€ë‹¹ ë¶ˆëŸ¬ì˜¬ ê²Œì‹œë¬¼ ìˆ˜. ë‚´ë¶€ APIì˜ limit íŒŒë¼ë¯¸í„°"
    )
with pag2:
    page = st.number_input(
        "í˜ì´ì§€ ë²ˆí˜¸",
        1, 100, 1, 1,
        help="page: ë¶ˆëŸ¬ì˜¬ í˜ì´ì§€ ë²ˆí˜¸. 1ë¶€í„° ì‹œì‘"
    )

# --- 5) ë‚´ë¶€ API í˜¸ì¶œ í•¨ìˆ˜ ---
@st.cache_data
def fetch_articles(region_tag, query, page, limit):
    """
    region_tag: "ë™ì´ë¦„-ì½”ë“œ" í˜•íƒœ
    query: ê²€ìƒ‰ì–´
    page: í˜ì´ì§€ ë²ˆí˜¸
    limit: í•œ í˜ì´ì§€ë‹¹ ì•„ì´í…œ ìˆ˜
    ë‚´ë¶€ APIëŠ” pageì™€ limitì„ ì´ìš©í•´ í˜ì´ì§•ëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì „ì²´ë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ pageë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    url = "https://www.daangn.com/kr/buy-sell/"
    params = {
        "in": region_tag,
        "search": query,
        "_data": "routes/kr.buy-sell._index",
        "page": page,
        "limit": limit
    }
    headers = {
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://www.daangn.com/kr/buy-sell/",
        "User-Agent": "Mozilla/5.0"
    }
    r = requests.get(url, params=params, headers=headers, verify=False)
    r.raise_for_status()
    return r.json()

# --- 6) ê²€ìƒ‰/ì¤‘ì§€ ì œì–´ ë²„íŠ¼ ---
with st.form('search form'):
    col_start, col_stop = st.columns([1,1])
    with col_start:
        start = st.form_submit_button("ğŸ” ê²€ìƒ‰ ì‹œì‘")
    with col_stop:
        stop = st.form_submit_button("ğŸ›‘ ê²€ìƒ‰ ì¤‘ì§€")

    # ì‚¬ìš©ìê°€ start ëˆ„ë¥´ë©´ is_searching ì¼œê³ , stop ëˆ„ë¥´ë©´ ë„ê¸°
    if start:
        st.session_state.is_searching = True
        st.session_state.stop_search = False
    if stop:
        st.session_state.stop_search = True

# --- 7) ê²€ìƒ‰ ì‹¤í–‰ & ì ì§„ì  ë Œë”ë§ ---
if st.session_state.is_searching:
    # í•­ìƒ ë¹ˆ df_mid ì •ì˜
    df_mid = pd.DataFrame()
    
    # ëŒ€ìƒ ì§€ì—­ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    if mode == "ì „êµ­ ê²€ìƒ‰":
        regions_to_search = (
            regions_df[["ê²€ìƒ‰ì–´","region_name","region_code"]]
            .drop_duplicates()
            .values.tolist()
        )
    else:
        sub_df = regions_df[regions_df["ê²€ìƒ‰ì–´"].isin(sub_sel)]
        regions_to_search = (
            sub_df[["ê²€ìƒ‰ì–´","region_name","region_code"]]
            .drop_duplicates()
            .values.tolist()
        )

    all_rows = []
    progress = st.progress(0)
    result_container = st.empty()
    # is_cloud = "STREAMLIT_APP_NAME" in os.environ
    total = len(regions_to_search)

    if parallel:
        # ë¡œì»¬: ë³‘ë ¬ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_map = {
                executor.submit(
                    fetch_articles,
                    f"{rname}-{rcode}", item, page, per_page
                ): (full, rname)
                for full, rname, rcode in regions_to_search
            }
            done = 0
            for future in as_completed(future_map):
                if st.session_state.stop_search:
                    break
                full, rname = future_map[future]
                data = future.result()
                arts = data.get("allPage", {}).get("fleamarketArticles", [])
                for art in arts:
                    created = pd.to_datetime(art["createdAt"])
                    if cutoff and created < cutoff:
                        continue
                    txt = (art["title"] + " " + art["content"]).lower()
                    if keywords and not all(kw in txt for kw in keywords):
                        continue
                    try:
                        price = int(float(art["price"]))
                    except:
                        price = None
                    sold = art.get("status", "").lower() == "closed"
                    if only_available and sold:
                        continue
                    if max_price > 0 and price is not None and price > max_price:
                        continue
                    if price is not None and price < min_price:
                        continue
                    all_rows.append({
                        "ì£¼ì†Œ": full,
                        "ë™/ì/ë©´": rname,
                        "ì œëª©": art["title"],
                        "ê°€ê²©": price,
                        "ë“±ë¡ì‹œê°„": created,
                        "íŒë§¤ì": art["user"]["nickname"],
                        "íŒë§¤ì™„ë£Œ": "ì˜ˆ" if sold else "ì•„ë‹ˆì˜¤",
                        "ë§í¬": art["href"],
                        "ì¸ë„¤ì¼": art.get("thumbnail")
                    })
                done += 1
                progress.progress(done / total)
                df_mid = (
                    pd.DataFrame(all_rows)
                    .drop_duplicates(subset=["ë§í¬"])  
                    .reset_index(drop=True)
                )
                result_container.dataframe(df_mid, use_container_width=True)
    else:
        # ì„œë²„(Cloud): ìˆœì°¨ ì²˜ë¦¬
        for idx, (full, rname, rcode) in enumerate(regions_to_search):
            if st.session_state.stop_search:
                break
            data = fetch_articles(f"{rname}-{rcode}", item, page, per_page)
            arts = data.get("allPage", {}).get("fleamarketArticles", [])
            for art in arts:
                created = pd.to_datetime(art["createdAt"])
                if cutoff and created < cutoff:
                    continue
                txt = (art["title"] + " " + art["content"]).lower()
                if keywords and not all(kw in txt for kw in keywords):
                    continue
                try:
                    price = int(float(art["price"]))
                except:
                    price = None
                sold = art.get("status", "").lower() == "closed"
                if only_available and sold:
                    continue
                if max_price > 0 and price is not None and price > max_price:
                    continue
                if price is not None and price < min_price:
                    continue
                all_rows.append({
                    "ì£¼ì†Œ": full,
                    "ë™/ì/ë©´": rname,
                    "ì œëª©": art["title"],
                    "ê°€ê²©": price,
                    "ë“±ë¡ì‹œê°„": created,
                    "íŒë§¤ì": art["user"]["nickname"],
                    "íŒë§¤ì™„ë£Œ": "ì˜ˆ" if sold else "ì•„ë‹ˆì˜¤",
                    "ë§í¬": art["href"],
                    "ì¸ë„¤ì¼": art.get("thumbnail")
                })
            progress.progress((idx + 1) / total)
            df_mid = (
                pd.DataFrame(all_rows)
                .drop_duplicates(subset=["ë§í¬"])  
                .reset_index(drop=True)
            )
            result_container.dataframe(df_mid, use_container_width=True)

    st.session_state.is_searching = False
    st.session_state.results_df = df_mid

# --- 8) ê²°ê³¼ ì¶œë ¥ & ë‹¤ìš´ë¡œë“œ ---
df_final = st.session_state.results_df
if not df_final.empty:
    st.success(f"âœ… ì´ {len(df_final)}ê±´ ê²€ìƒ‰ ì™„ë£Œ")
    csv = df_final.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button("ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ", data=csv, file_name="results.csv", mime="text/csv")
    if mode == "ì§€ì—­ ê²€ìƒ‰":
        st.markdown("### ì¹´ë“œ ë·°")
        recs = df_final.to_dict('records')
        for i in range(0, len(recs), 3):
            cols = st.columns(3, gap="small")
            for j, rec in enumerate(recs[i:i+3]):
                with cols[j]:
                    thumb = rec.get("ì¸ë„¤ì¼")
                    if thumb:
                        st.image(thumb, use_container_width=True)
                    st.markdown(f"**{rec['ì œëª©']}**")
                    st.markdown(f"- ğŸ’° {rec['ê°€ê²©']}ì›")
                    st.markdown(f"- ğŸ“ {rec['ë™/ì/ë©´']} | {rec['ì£¼ì†Œ']}")
                    st.markdown(f"- ğŸ•’ {rec['ë“±ë¡ì‹œê°„'].strftime('%Y-%m-%d %H:%M')}")
                    st.markdown(f"- ğŸ‘¤ {rec['íŒë§¤ì']} | íŒë§¤ì™„ë£Œ: {rec['íŒë§¤ì™„ë£Œ']}")
                    st.markdown(f"[ğŸ”— ìƒì„¸ë³´ê¸°]({rec['ë§í¬']})")
                    st.markdown("---")
