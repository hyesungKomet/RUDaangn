# streamlit_app.py
import streamlit as st
import pandas as pd
import requests
from dateutil.relativedelta import relativedelta
import urllib3

# í˜ì´ì§€ ë ˆì´ì•„ì›ƒ
st.set_page_config(layout="wide")
st.title("ARE YOU DAANGN?")

# SSL ê²½ê³  ë¬´ì‹œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
if "is_searching" not in st.session_state:
    st.session_state.is_searching = False
if "stop_search" not in st.session_state:
    st.session_state.stop_search = False
if "results_df" not in st.session_state:
    st.session_state.results_df = pd.DataFrame()

# --- 1) ì§€ì—­ ì½”ë“œ CSV ë¡œë“œ ---
@st.cache_data
def load_regions():
    df = pd.read_csv("address_with_all_codes.csv", dtype=str)
    df = df.drop_duplicates(subset=["ê²€ìƒ‰ì–´", "region_name", "region_code"]).copy()
    df["ê´‘ì—­"] = df["ê²€ìƒ‰ì–´"].map(lambda x: x.split()[0])
    MAJORS = [
        "ì„œìš¸íŠ¹ë³„ì‹œ","ë¶€ì‚°ê´‘ì—­ì‹œ","ëŒ€êµ¬ê´‘ì—­ì‹œ","ì¸ì²œê´‘ì—­ì‹œ","ê´‘ì£¼ê´‘ì—­ì‹œ",
        "ëŒ€ì „ê´‘ì—­ì‹œ","ìš¸ì‚°ê´‘ì—­ì‹œ","ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ","ê²½ê¸°ë„","ê°•ì›íŠ¹ë³„ìì¹˜ë„",
        "ì¶©ì²­ë¶ë„","ì¶©ì²­ë‚¨ë„","ì „ë¼ë¶ë„","ì „ë¼ë‚¨ë„","ê²½ìƒë¶ë„","ê²½ìƒë‚¨ë„","ì œì£¼íŠ¹ë³„ìì¹˜ë„"
    ]
    return df, MAJORS

regions_df, MAJORS = load_regions()

# --- 2) ê²€ìƒ‰ ëª¨ë“œ, ì…ë ¥, í‚¤ì›Œë“œ, ë“±ë¡ê¸°ê°„, íŒë§¤ìƒíƒœ UI ---
mode_col, item_col, key_col, period_col, sale_col = st.columns([1,4,3,2,1])
with mode_col:
    mode = st.radio("ğŸ” ëª¨ë“œ", ["ì „êµ­ ê²€ìƒ‰", "ì§€ì—­ ê²€ìƒ‰"], index=1, horizontal=True)
with item_col:
    item = st.text_input("ì°¾ì„ ë¬¼í’ˆ (ex: ë…¸íŠ¸ë¶)", "")
with key_col:
    kws = st.text_input(
        "í•µì‹¬ í‚¤ì›Œë“œ - ì‰¼í‘œë¡œ êµ¬ë¶„ (ex: ê²Œì´ë°, 3060)",
        help="ì œëª©+ë‚´ìš©ì— ëª¨ë‘ í¬í•¨ëœ í•­ëª©ë§Œ í•„í„°ë§"
    )
    keywords = [w.strip().lower() for w in kws.split(",") if w.strip()]
with period_col:
    period = st.selectbox(
        "ë“±ë¡ ê¸°ê°„",
        ["ì „ì²´", "1ì¼", "7ì¼", "1ê°œì›”", "3ê°œì›”", "6ê°œì›”", "1ë…„"],
        help="ê¸°ê°„ë³„ë¡œ ìµœê·¼ ë“±ë¡ê¸€ë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤"
    )
with sale_col:
    only_available = st.checkbox("íŒë§¤ì¤‘ë§Œ ë³´ê¸°", value=False, help="íŒë§¤ì™„ë£Œëœ ê¸€ì„ ì œì™¸í•©ë‹ˆë‹¤.")

# ê¸°ê°„ ì»·ì˜¤í”„ ê³„ì‚°
now = pd.Timestamp.now(tz="Asia/Seoul")
if period == "1ì¼": cutoff = now - relativedelta(days=1)
elif period == "7ì¼": cutoff = now - relativedelta(days=7)
elif period == "1ê°œì›”": cutoff = now - relativedelta(months=1)
elif period == "3ê°œì›”": cutoff = now - relativedelta(months=3)
elif period == "6ê°œì›”": cutoff = now - relativedelta(months=6)
elif period == "1ë…„": cutoff = now - relativedelta(years=1)
else: cutoff = None

# --- 3) ì§€ì—­ ê²€ìƒ‰ í•„í„° ---
sub_sel = []
if mode == "ì§€ì—­ ê²€ìƒ‰":
    with st.expander("ì§€ì—­ í•„í„° ì„¤ì •", expanded=True):
        maj_sel = st.multiselect("ê´‘ì—­ ì„ íƒ", MAJORS, default=MAJORS[:3])
        sub_df = regions_df[regions_df["ê´‘ì—­"].isin(maj_sel)]
        sub_opts = sorted(sub_df["ê²€ìƒ‰ì–´"].unique())
        sub_sel = st.multiselect("ë™/ì/ë©´ ì„ íƒ (ë¹„ì›Œë‘ë©´ ëª¨ë‘)", sub_opts)
        if not sub_sel:
            sub_sel = sub_opts.copy()

# --- 4) í˜ì´ì§€ë„¤ì´ì…˜ ---
pag1, pag2 = st.columns(2)
with pag1:
    per_page = st.number_input("í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜", 5, 50, 10, 5)
with pag2:
    page = st.number_input("í˜ì´ì§€ ë²ˆí˜¸", 1, 100, 1, 1)

# --- 5) ë‚´ë¶€ API í˜¸ì¶œ í•¨ìˆ˜ (SSL ê²€ì¦ ë¹„í™œì„±í™”) ---
@st.cache_data
def fetch_articles(region_tag, query, page, limit):
    url = "https://www.daangn.com/kr/buy-sell/"
    params = {"in": region_tag, "search": query,
              "_data": "routes/kr.buy-sell._index",
              "page": page, "limit": limit}
    headers = {"Accept": "application/json, text/plain, */*",
               "Referer": "https://www.daangn.com/kr/buy-sell/",
               "User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, params=params, headers=headers, verify=False)
        r.raise_for_status()
        return r.json()
    except Exception as e:
        st.error(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
        return {}

# --- 6) ì œì–´ ë²„íŠ¼ ---
col_start, col_stop = st.columns([1,1])
with col_start:
    if st.button("ğŸ” ê²€ìƒ‰ ì‹œì‘"):
        st.session_state.is_searching = True
        st.session_state.stop_search = False
with col_stop:
    if st.button("ğŸ›‘ ê²€ìƒ‰ ì¤‘ì§€"):
        st.session_state.stop_search = True

# --- 7) ê²€ìƒ‰ ì‹¤í–‰ & ì ì§„ì  ë Œë”ë§ ---
if st.session_state.is_searching:
    df_mid = pd.DataFrame()
    if mode == "ì „êµ­ ê²€ìƒ‰":
        regions_to_search = (
            regions_df[["ê²€ìƒ‰ì–´","region_name","region_code"]]
            .drop_duplicates().values.tolist())
    else:
        sub_df = regions_df[regions_df["ê²€ìƒ‰ì–´"].isin(sub_sel)]
        regions_to_search = (
            sub_df[["ê²€ìƒ‰ì–´","region_name","region_code"]]
            .drop_duplicates().values.tolist())

    total = len(regions_to_search)
    progress = st.progress(0)
    result_container = st.empty()
    all_rows = []

    for idx, (full, rname, rcode) in enumerate(regions_to_search):
        if st.session_state.stop_search:
            st.warning("ğŸ›‘ ê²€ìƒ‰ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break

        tag = f"{rname}-{rcode}"
        data = fetch_articles(tag, item, page, per_page)
        arts = data.get("allPage", {}).get("fleamarketArticles", [])

        for art in arts:
            created = pd.to_datetime(art["createdAt"])
            if cutoff and created < cutoff:
                continue
            txt = (art["title"] + " " + art["content"]).lower()
            if keywords and not all(kw in txt for kw in keywords):
                continue
            try:
                price_int = int(float(art["price"]))
            except:
                price_int = None
            sold = (art.get("status","" ) .lower() == "closed")
            if only_available and sold:
                continue

            all_rows.append({
                "ì£¼ì†Œ": full,
                "ë™/ì/ë©´": rname,
                "ì½”ë“œ": rcode,
                "ì œëª©": art["title"],
                "ê°€ê²©": price_int,
                "ë“±ë¡ì‹œê°„": created,
                "íŒë§¤ì": art["user"]["nickname"],
                "íŒë§¤ì™„ë£Œ": "ì˜ˆ" if sold else "ì•„ë‹ˆì˜¤",
                "ë³¸ë¬¸ë‚´ìš©": art["content"],
                "ë§í¬": art["href"],
                "ì¸ë„¤ì¼": art["thumbnail"]
            })

        df_mid = pd.DataFrame(all_rows)
        if not df_mid.empty:
            df_mid = df_mid.drop_duplicates(subset=["ë§í¬"]).reset_index(drop=True)
            result_container.dataframe(
                df_mid.drop(columns=["ì½”ë“œ", "ì¸ë„¤ì¼"], errors="ignore"),
                use_container_width=True
            )
        progress.progress((idx + 1) / total)

    st.session_state.is_searching = False
    st.session_state.results_df = df_mid

# --- 8) ê²°ê³¼ ì¶œë ¥ & ì¹´ë“œë·° & ë‹¤ìš´ë¡œë“œ ---
df_final = st.session_state.results_df
if not df_final.empty:
    st.success(f"âœ… ì´ {len(df_final)}ê±´ ê²€ìƒ‰ ì™„ë£Œ")

    csv_data = df_final.drop(columns=["ì¸ë„¤ì¼"], errors="ignore").to_csv(
        index=False, encoding="utf-8-sig").encode("utf-8-sig")
    st.download_button(
        label="ğŸ“¥ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
        data=csv_data,
        file_name="daangn_search_results.csv",
        mime="text/csv"
    )

    if mode == "ì§€ì—­ ê²€ìƒ‰":
        st.markdown("### ì¹´ë“œ ë·°")
        records = df_final.to_dict('records')
        for i in range(0, len(records), 3):
            cols = st.columns(3, gap="small")
            for j, rec in enumerate(records[i:i+3]):
                with cols[j]:
                    thumb = rec.get("ì¸ë„¤ì¼")
                    if thumb:
                        st.image(thumb, use_container_width=True)
                    st.markdown(f"**{rec['ì œëª©']}**")
                    st.markdown(f"- ğŸ’° {rec['ê°€ê²©']}ì›")
                    st.markdown(f"- ğŸ“ {rec['ë™/ì/ë©´']} | {rec['ì£¼ì†Œ']}")
                    st.markdown(f"- ğŸ•’ {rec['ë“±ë¡ì‹œê°„'].strftime('%Y-%m-%d %H:%M')}")
                    st.markdown(f"- ğŸ‘¤ {rec['íŒë§¤ì']} | íŒë§¤ì™„ë£Œ: {rec['íŒë§¤ì™„ë£Œ']}")
                    st.markdown(f"[ğŸ”— ìƒì„¸ë³´ê¸°]({rec['ë§í¬']})")
                    st.markdown("---")
